{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Classifying penguin species with PyTorch\n",
    "\n",
    "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png\" width=\"750\" />\n",
    "\n",
    "\n",
    "Artwork by @allison_horst\n",
    "\n",
    "In this exercise, we will use the python package [``palmerpenguins``](https://github.com/mcnakhaee/palmerpenguins) to supply a toy dataset containing various features and measurements of penguins.\n",
    "\n",
    "We have already created a PyTorch dataset which yields data for each of the penguins, but first we should examine the dataset and see what it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: look at the data\n",
    "In the following code block, we import the ``load_penguins`` function from the ``palmerpenguins`` package.\n",
    "\n",
    "- Call this function, which returns a single object, and assign it to the variable ``data``.\n",
    "  - Print ``data`` and recognise that ``load_penguins`` has returned a ``pandas.DataFrame``.\n",
    "- Consider which features it might make sense to use in order to classify the species of the penguins.\n",
    "  - You can print the column titles using ``pd.DataFrame.keys()``\n",
    "  - You can also obtain useful information using ``pd.DataFrame.Series.describe()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['species', 'island', 'bill_length_mm', 'bill_depth_mm',\n",
      "       'flipper_length_mm', 'body_mass_g', 'sex', 'year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = load_penguins()\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now disuss the features we will use to classify the penguins' species, and populate the following list together:\n",
    "- ...\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: creating a ``torch.utils.data.Dataset``\n",
    "\n",
    "All PyTorch dataset objects are subclasses of the ``torch.utils.data.Dataset`` class. To make a custom dataset, create a class which inherits from the ``Dataset`` class, implement some methods (the Python magic (or dunder) methods ``__len__`` and ``__getitem__``) and supply some data.\n",
    "\n",
    "Spoiler alert: we've done this for you already in ``src/ml_workshop/_penguins.py``.\n",
    "\n",
    "- Open the file ``src/ml_workshop/_penguins.py``.\n",
    "- Let's examine, and discuss, each of the methods together.\n",
    "  - ``__len__``\n",
    "    - What does the ``__len__`` method do?\n",
    "    - ...\n",
    "  - ``__getitem__``\n",
    "    - What does the ``__getitem__`` method do?\n",
    "    - ...\n",
    "- Review and discuss the class arguments.\n",
    "  - ``input_keys``— ...\n",
    "  - ``target_keys``— ...\n",
    "  - ``train``— ...\n",
    "  - ``x_tfms``— ...\n",
    "  - ``y_tfms``— ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Obtaining training and validation datasets\n",
    "\n",
    "- Instantiate the penguin dataloader.\n",
    "  - Make sure you supply the correct column titles for the features and the targets.\n",
    "- Iterate over the dataset\n",
    "    - Hint:\n",
    "        ```python\n",
    "        for features, targets in dataset:\n",
    "            # print the features and targets here\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_workshop import PenguinDataset\n",
    "\n",
    "features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex']\n",
    "\n",
    "data_set = PenguinDataset(\n",
    "    input_keys=[\"bill_length_mm\", \"body_mass_g\"],\n",
    "    target_keys=[\"species\"],\n",
    "    train=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can we give these items to a neural network, or do they need to be transformed first?\n",
    "  - Short answer: no, we can't just pass tuples of numbers or strings to a neural network.\n",
    "    - We must represent these data as ``torch.Tensor``s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Applying transforms to the data\n",
    "\n",
    "A common way of transforming inputs to neural networks is to apply a series of transforms using ``torchvision.transforms.Compose``. The ``Compose`` object takes a list of callable objects and applies them to the incoming data.\n",
    "\n",
    "These transforms can be very useful for mapping between file paths and tensors of images, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "from torch import tensor, eye\n",
    "\n",
    "# Apply the transforms we need to the PenguinDataset to get out inputs\n",
    "# targets as Tensors.\n",
    "\n",
    "features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex']\n",
    "species = ['Gentoo', 'Chinstrap', 'Adelie']\n",
    "\n",
    "train_set = PenguinDataset(\n",
    "    input_keys=features,\n",
    "    target_keys=['species'],\n",
    "    train=True,\n",
    "    x_tfms=Compose([tensor, lambda x: x.float()]),\n",
    "    y_tfms=Compose([lambda x: eye(3)[species.index(x[0])]])\n",
    ")\n",
    "\n",
    "valid_set = PenguinDataset(\n",
    "    input_keys=features,\n",
    "    target_keys=['species'],\n",
    "    train=False,\n",
    "    x_tfms=Compose([tensor, lambda x: x.float()]),\n",
    "    y_tfms=Compose([lambda x: eye(3)[species.index(x[0])]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Creating ``DataLoaders``—and why\n",
    "\n",
    "- Once we have created a ``Dataset`` object, we then wrap it in a ``DataLoader``.\n",
    "  - The ``DataLoader`` object allows us to put our inputs and targets in mini-batches, which makes for more efficient training.\n",
    "    - Note: rather than suppling one input-target pair to the model at a time, we supply \"mini-batches\" of these data at once.\n",
    "    - The number of items we supply at once is called the batch size.\n",
    "  - The ``DataLoader`` also randomly shuffles the data each epoch (when training).\n",
    "  - It allows us to load different mini-batches in parallel, which can be very useful for larger datasets and images that can't all fit in memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batchsize = 16\n",
    "\n",
    "train_loader = DataLoader(train_set,shuffle=True,batch_size=batchsize, \n",
    "                          drop_last=True) # ensures batch size is always consistent\n",
    "valid_loader = DataLoader(valid_set,shuffle=True,batch_size=batchsize)\n",
    "\n",
    "# Create training and validation DataLoaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Creating a neural network in PyTorch\n",
    "\n",
    "Here we will create our neural network in PyTorch, and have a general discussion on clean and messy ways of going about it.\n",
    "\n",
    "- First, we will create quite an ugly network to highlight how to make a neural network in PyTorch on a very basic level.\n",
    "- We will then discuss a trick for making the print-out nicer.\n",
    "- Finally, we will discuss how the best approach would be to write a class where various parameters (e.g. number of layers, droupout probabilities, etc.) are passed as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.nn import BatchNorm1d, Linear, ReLU, Dropout, Sequential\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class FCNet(Sequential):\n",
    "    \"\"\"Fully-connected neural network.\"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        # easy way to construct layers\n",
    "        super().__init__(BatchNorm1d(5), # normalization, we can pad each layer with this\n",
    "                        Linear(5, 16),\n",
    "                        ReLU(),\n",
    "                        Linear(16, 8),\n",
    "                        ReLU(),\n",
    "                        Linear(8, 3))\n",
    "        \n",
    "model = FCNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Selecting a loss function\n",
    "\n",
    "- Binary cross-entropy is about the most common loss function for classification.\n",
    "  - Details on this loss function are available in the [PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html).\n",
    "- Let's instantiate it together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "\n",
    "loss_func = BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Selecting an optimiser\n",
    "\n",
    "While we talked about stochastic gradient descent in the slides, most people use the so-called [Adam optimiser](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html).\n",
    "\n",
    "You can think of it as a more complex and improved implementation of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimiser and give it the model's parameters.\n",
    "from torch.optim import Adam\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Writing basic training and validation loops\n",
    "\n",
    "- Before we jump in and write these loops, we must first choose an activation function to apply to the model's outputs.\n",
    "  - Here we are going to use the softmax activation function: see [the PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html).\n",
    "  - For those of you who've studied physics, you may be remininded of the partition function in thermodynamics.\n",
    "  - This activation function is good for classifcation when the result is one of ``A or B or C``.\n",
    "    - It's bad if you even want to assign two classification to one images—say a photo of a dog _and_ and cat.\n",
    "  - It turns the raw outputs, or logits, into \"psuedo probabilities\", and we take our prediction to be the most probable class.\n",
    "\n",
    "- We will write the training loop together, then you can go ahead and write the (simpler) validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from numpy import mean\n",
    "\n",
    "from torch import no_grad\n",
    "\n",
    "@no_grad()\n",
    "def get_accuracy(pred: Tensor, target: Tensor):\n",
    "    \n",
    "    decision = pred.argmax(dim=1)\n",
    "    \n",
    "    return (decision == target.argmax(dim=1)).float().mean()\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: Module,\n",
    "    train_loader: DataLoader,\n",
    "    optimiser: Adam,\n",
    "    loss_func: BCELoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Train ``model`` for once epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The neural network.\n",
    "    train_loader : DataLoader\n",
    "        Training dataloader.\n",
    "    optimiser : Adam\n",
    "        The optimiser.\n",
    "    loss_func : BCELoss\n",
    "        Binary cross-entropy loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        A dictionary of metrics.\n",
    "\n",
    "    \"\"\"\n",
    "    metrics = {'loss':[], 'accuracy':[]}\n",
    "    model.train()\n",
    "    \n",
    "    for batch, targets in train_loader:\n",
    "        optimiser.zero_grad() # deletes gradients from past epochs\n",
    "        \n",
    "        # activation function for output\n",
    "        # (N, num_classes)\n",
    "        preds = model(batch).softmax(dim=1) # sum over dim 1 = 1\n",
    "        \n",
    "        loss = loss_func(preds, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimiser.step()\n",
    "        \n",
    "        metrics['loss'].append(loss.item()) # we could do running sum instead of list if large epochs\n",
    "        metrics['accuracy'].append(get_accuracy(preds, targets))\n",
    "        \n",
    "    return {met : mean(values) for met, values in metrics.items()}\n",
    "\n",
    "@no_grad()\n",
    "def validate_one_epoch(\n",
    "    model: Module,\n",
    "    valid_loader: DataLoader,\n",
    "    loss_func: BCELoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Validate ``model`` for a single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The neural network.\n",
    "    valid_loader : DataLoader\n",
    "        Training dataloader.\n",
    "    loss_func : BCELoss\n",
    "        Binary cross-entropy loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        Metrics of interest.\n",
    "\n",
    "    \"\"\"\n",
    "    metrics = {'loss':[], 'accuracy':[]}\n",
    "    model.eval()\n",
    "    \n",
    "    for batch, targets in valid_loader:\n",
    "        preds = model(batch).softmax(dim=1) # sum over dim 1 = 1\n",
    "        \n",
    "        loss = loss_func(preds, targets)\n",
    "        \n",
    "        metrics['loss'].append(loss.item()) # we could do running sum instead of list if large epochs\n",
    "        metrics['accuracy'].append(get_accuracy(preds, targets))\n",
    "        \n",
    "    return {met : mean(values) for met, values in metrics.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Training, extracting and plotting metrics\n",
    "\n",
    "- Now we can train our model for a specified number of epochs.\n",
    "  - During each epoch the model \"sees\" each training item once.\n",
    "- Append the training and validation metrics to a list.\n",
    "- Turn them into a ``pandas.DataFrame``\n",
    "  - Note: You can turn a ``List[Dict[str, float]]``, say ``my_list`` into a ``DataFrame`` with ``DataFrame(my_list)``.\n",
    "- Use Matplotlib to plot the training and validation metrics as a function of the number of epochs.\n",
    "\n",
    "We will begin the code block together before you complete it independently.  \n",
    "After some time we will go through the solution together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   loss_train  accuracy_train  loss_valid  accuracy_valid\n",
      "0    0.622891        0.338235    0.639671        0.348958\n",
      "1    0.622996        0.341912    0.641569        0.333333\n",
      "2    0.622988        0.349265    0.643010        0.328125\n",
      "3    0.622543        0.349265    0.641861        0.333333\n",
      "4    0.622014        0.345588    0.641227        0.333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "epochs = 5\n",
    "\n",
    "train_metrics, valid_metrics = [], []\n",
    "\n",
    "for _ in range(epochs):\n",
    "    train_metrics.append(train_one_epoch(model, train_loader, optim, loss_func))\n",
    "    valid_metrics.append(validate_one_epoch(model, valid_loader, loss_func))\n",
    "    \n",
    "train_metrics = pd.DataFrame(train_metrics)\n",
    "valid_metrics = pd.DataFrame(valid_metrics)\n",
    "\n",
    "print(train_metrics.join(valid_metrics, lsuffix=\"_train\", rsuffix=\"_valid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Visualise some results\n",
    "\n",
    "Let's do this part together—though feel free to make a start on your own if you have completed the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#axes[0].plot(train_metrics.loss, label=\"Train\")\n",
    "#axes[0].plot(valid_metrics.loss, label=\"Valid\")\n",
    "\n",
    "#for axis in axes.ravel():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
